---
title: nju-course mlt
copyright: true
top: 0
reward: false
mathjax: true
date: 2019-09-19 12:02:37
tags:
- nju
- course
- mlt
categories:
- [course, nju, mlt]
---
## 1. BasicInequality
有哪些不等式？
这些不等式是为了解决什么样的问题的？

凸函数：杰森不等式说明函数的期望大于期望的函数，这说明了什么？

a. 基本不等式
- 杰森不等式
> 杰森不等式即变量的期望的函数小于函数的期望
- Markov不等式
> 将$P{X \geq a}$的概率与期望联系在一起
- chernoff不等式
> 将指数以及指数的性质引入。
- 大数定理
> 说明了大概率事件的问题
- 信息熵不等式
> 有什么用处，暂时未知

b. Concentration
> 研究期望与样本均值之间的关系

c. Martingale不等式
> 结论很有意思，得到了函数与函数期望之间关系的不等式
McDiarmid不等式
A martingale is a sequence of random variables for which, at a particular time in the realized sequence, the expectation of the next value in the sequence is equal to the present observed value even given knowledge of all prior observed values.


## 3. PAC理论
PAC理论是什么？
主要应对的是什么问题？

PAC是概率近似正确的表达式。首先对于衡量的指标，泛化误差也就是在未知样本上预测结果与真实结果的差异性，如果预测函数是真实的概念类，那么泛化误差应该为0. 经验误差是在已知有限样本数据上的真实与否的一个估计值。
PAC学习假设数据原本是存在某一个分布的，而且手上的数据集是通过iid采样得到的。那么，
经验误差的期望就应该与泛化误差相等，由于iid条件和期望的线性性。

注意到，这里的经验误差的计算方式非常简单。

对于PAC可学习，即我们希望尽可能学习到真实的概念，此时就提出了以$1-\delta$ 满足泛化误差小于某个常数

但是，我们知道学习任务实际上是有两类的：
第一类，简单的，可以在假设空间中学习到概念，此时使用的就是PAC理论
第二类，困难的，不可以在假设空间中学习到概念。这种情况往往就是假设空间不合理，不可知PAC可学习。

对于不可知PAC可学习，此时采用的方法就是使用集中不等式，有给出期望与数据样本的均值之间的关系，所以可以采用就将泛化误差与训练误差之间联系在一起。
这也是为什么实际上做的时候，训练样本不断使样本的误差达到最小，从而在未知样本上保持好的性能。

有什么用？
某任务在什么样的条件下可学得较好的模型？需要多少训练样例才能获得较好的模型？

因为机器学习是在假设空间中寻找概念，所以最重要的一个因素就是衡量假设空间的复杂度。
> 注意在实际问题中，假设空间其实就是问题的定义域。假设空间可分就是目标概念在假设空间中可以找到，然后实际上就是>证明算法，若算法给定就选该算法，若算法未给定，一般考虑选择经验风险最小化的算法。
> 然后根据定义即证明了可分
如果假设空间复杂度有限，用个数来衡量
由前面的PAC可学习公式我们就可以得到样本复杂度。
一般来说都解题步骤就是首先得到算法，然后图形结合得到泛化整个集合的泛化误差，然后通过独立同分布得到与单个泛化误差之间的关系，最终即可得到样本复杂度。
## 4. 复杂度
- 为什么可以用PAC理论来得到学习算法？
当认为处理每个样本的时间是常数，则算法的复杂度就与样本复杂度相关。又由PAC定义，如果概念在假设空间中可以由假设空间复杂度得到样本复杂度。
- 如果假设空间无限，引入假设空间，增长函数和VC维。
> 无限假设空间的另一种理解方法就是定义域往往是无穷的
无限假设空间被有限大小的数据集限制。
增长函数即指该概念可以出现多少不同类的情况，则增长函数一定小于H_D
VC维就是log_2 {增长函数}

- 增长函数可以通过不等式放缩。
- 研究数据分布相关的假设空间复杂度
> 考虑到现实中的某些标记不一定是真实标记，此时选择事先考虑随机噪声的假设更好。为什么这样更好？因为这里也是往往认为这种时间是具有一定分布的，即是随机噪声。
> 常见的噪声的取值为0.5的概率取某值，当然在其他情况下，噪声可能有其他的分布。
> 为什么要用上界来衡量？如同增长函数的感觉，上界来衡量的感觉就是去找最大的情况，而所有做的过程都是这样来做的。

注意有两种复杂度：
第一种，经验复杂度，即在随机噪声上的期望
第二种，分布复杂度，即在具体分布上的期望

几个复杂度求解的例子。

作业1-习题3：
求R复杂度时注意关于期望的运算最好使用期望原本的公式。

## 5. 泛化性
机器学习模型在未见数据上的预测能力。泛化性的好坏可由泛化误差界来表示。
所以泛化性的评价标准就是泛化误差。
由前面的分析可知，泛化误差区分可分和不可分两种情况。
对于可分的情况，样本复杂度是关于名同事我们还可以知道泛化误差的收敛速率
对于不可分的情况，此时用的是误差经验最小化来查看结果。
> 注意不可分与假设空间无限两个概念不要弄混。

无限空间泛化误差界：
- 基于VC维的泛化误差界
- 基于Rademacher复杂度的泛化误差界

在研究SVM的VC维时，引入间隔理论，即

假设x的直径的最大长度为r，权向量的最大模为某个定值。最终根据不等式的放缩、期望以及期望的性质最终可得大小关系式。

评价：所求的泛化性是对所欲算法都适用的，与具体的学习算法无关。
## 6. 稳定性
$\zeta_D$: 算法基于样本集返回的假设。
z: 代指实际的标签(x,y)
关于移除样本的$\beta$-稳定性和替换样本$\gamma$。

因为稳定性的定义是两个误差的绝对值，所以趋于稳定即这个值越小越好。那么想要基于稳定性的算法具有收敛性，从某个角度理解就是需要该算法能够随着样本的增大而缩小，因为只有样本容量这一个大小。

定理6.1利用McDiamnd不等式给出了基于稳定性所学的假设的泛化误差界。
给出了泛化误差界与训练误差界之间的关系。

- 稳定性与可学习性之间有什么关系？
稳定性关注点是算法输出的假设的误差，并不考虑空间中所有假设。
一个结论：若果学习算法是ERM且稳定的，那么假设空间是PAC可学习的。
为什么？
因为从稳定性的定义来看就是损失函数上套了一层。

次梯度，即实际函数与函数的点的切线的距离。
这里的关键是有次梯度的关系表达式，即泰勒级数的一阶展开式。将$B_F$和稳定性定义，以及梯度联系在一起。
特别说明地是，这里在证明稳定性的上界的时候可以发现，首先是证明了$\Delta h$的最大值的上界。
然后再用$\delta$可容许将稳定性的误差放缩为$\Delta h$，最终得到稳定性的范围。
注意这里是讨论了基于核正则化算法的稳定性。

注意到定理6.1,可以发现最关键的就是分析损失函数的上界以及替换误差.

课文中只给出了基于再生核的泛化误差界，没有给出其他。
还有一点就是本文的讨论都是基于均匀稳定性导出的泛化误差界，但均匀稳定性并不是算法奏效的必要条件，
## 7. AdaBoost分析
集成学习即利用某种规则将各个学习结果进行整合从而获得比单个学习器显著优越的泛化性能。

弱学习器的定义：弱学习器是指在多项式时间内存在一个学习器，其错误率低于50%，即存在比随机猜略好的学习方法。

简单版的AdaBoost算法
在每一轮迭代中，产生一个弱学习器ht,该学习器比随机猜的性能略好，然后根据产生的弱学习器ht，每一个样本被赋予一个权重，如果样本点被正确地分类，那么它的权重越小，表明在下一轮训练中被选中的概率越低。相反，如果某样本点没有被准确地分类，那么它的权重就会提高，然后更显样本集并用于下一轮的学习训练，整个训练过程如此迭代，最后将所有得到的弱分类器通过权重结合起来。

这里是使用了最优化，使泛化误差去极小值，最终得到权值的更新公式。

**泛化误差反应的是在未知结果上的好坏，而函数空间复杂度则反应的是学习的程度如何，比如是否过拟合之类的信息**

**奥卡姆剃刀原理**：在于经验数据一致的模型中，模型越简单，泛化性越好；模型越复杂，泛化性越差。

- AdaBoost间隔 
AdaBoost间隔理论的定义：
间隔定义为正确/错误分类样本(x,y)的基分类器权重之差，在一定程度上了反映了分类器的可信度。
有种执行区间的感觉。

AdaBoost间隔结论
学习器在真实分布上的泛化错误率与在训练集上的间隔有关，与训练集的大小，基学习器的个数有关。
所以说明了AdaBoost算法但训练错误率时不停止训练，可以进一步增大间隔，从而提高学习方法的泛化性而没有过拟合。
也就是AdaBoost算法不易引起过拟合现象。想想真的是超级神奇了。

- AdaBoost VC维泛化误差性
由VC维得到结论：随着迭代轮数的增加，学习器函数空间复杂度增加，从而导致发生过拟合风险。不过这是有问题的。

- AdaBoost Rademaer泛化误差性
可以发现，这里所用的都是利普西子条件来分析的。
## 8. 一致性
结构风险，1范数-稀疏性，2范数-平均性

一致性这里研究的就是模型和测试的结果之间是否一致，至少在理论上提供保证。

KNN 经验风险最小化。
Tree 拆分法

所以这里的问题的起点变为了首先给定样本空间和标记空间。D是这两个空间的笛卡尔积的一个联合分布。分布D可分解为边缘分布和条件概率，为什么？
> 因为联合分布的定义就是边缘分布乘以条件概率

若函数给出一个分类器或决策函数，其错误率定义为
> R(g)=Pr[g(X)!=Y]=E[I_{g(X)!=Y}]

那么，考虑最优分类器及其性质
> 这里应该有理论上最优和基于后验最优。

只考虑所有可测函数，选出最符合后验概率的，作为最优。

> 注意这里给定任意的损失函数来做经验风险最小化。
> 首先按照条件期望展开，然后把g(X)即分类器当做变量。对损失求极值，由此可以得到最优分类器和最优风险。
此时就被称为贝叶斯风险和贝叶斯分类器。

> 注意指示函数非凸不连续做不到效果，所以建议使用对数，指数和sigmoid函数。
如果是凸函数可以得到全局最优解。
> 各点最小，全局最小。

- 这里的$\eta(x)=Pr(Y=1|X=x)$
一种经典的条件概率估计方法：Plug-in学习方法。
> 该学习方法是通过训练数据集来估计$\eta(x)$, 即学习器$g(x)=I[\widehat\eta(x) > \frac {1}{2}]$
> 这里是训练集预测的结果差异
实际上理解就是少数服从多数

然后，将Plug-in学习器与最优分类器的错误率之差建立了与两个n(x)的差的期望

- 拆分算法
> 将示例空间划分为多个互不相交的单元格A1,A2,...。对每个单元格投票的方法赋予标记，还是少数服从多数。

A(X)这里表示包含X的单元格。N(X)表示训练集中与示例X落入同一单元格的实例数。

- Box算法
- 随机森林一致性

- 替换损失函数一致性
> 替换函数一般都需要是凸函数。
> 替换函数一致性是指替换损失的最优解接近于0,1损失的最优解
> 替代函数的种类： 
> 1. 最小二乘损失函数 $(1-t)^2$
> 2. Hinge损失函数 $max(0,1-t)$
> 3. 指数损失函数 $e^{-t}$
> 4. 对数损失函数 $log(1+e^{-t})$
> 5. 平方Hinge损失函数 $(max(0,1-t))^2$

## 9. 优化
> 监督学习目的想在未知样本中能预测到好的结果，实际上就是测试数据上的期望的性能。往往思考的时候添加个损失函数，所以就变成了是带损失的期望风险最小化。

### 9.1 凸优化
> 凸优化求解经验风险最小化
几个概念
1. 凸集合
2. 凸函数的几种判定方式
> ? Jensen不等式有什么性质
> 凸函数能保证局部最优解就是全局最优解。
> 一阶条件的理解：可以想象成梯度的等温线然后来做。
> 一般地，Affine,Exponential,Powers,Powers of absolute value, Negative entropy都是凸函数
> 几种合起来的套路：
> 非负加权、线性加权、max、sup(只需要自由变元满足即可)
3. 共轭函数
> 注意：无论原函数f是否为凸函数，共轭函数f*都为凸
4. 对偶问题
KKT条件：
- 原始问题约束
- 对偶问题约束
- 互补松弛条件
- 原函数梯度为0
5. 优化方法
对于梯度下降，目的就是为了分析其在凸函数情况下的收敛速率。
> ?

> ? 所以说可以认为凸优化在这里就对应于定步长的梯度下降。

> 凸优化再思考，凸优化为什么表达式是最优化目标min,小于等于，还有必须是纺织函数；
因为Jensen不等式，所以就有加权函数值小于等于函数值加权，所以就能找到最优化目标。

### 9.2 随机优化
> 随机优化是类大的引题，随机优化是一种方法，一种思想。然后在机器学习中就对应于经验风险最小化部分。

随机优化求解有两种方法：样本平均近似和随机近似。

#### 样本平均近似
机器学习领域的经验风险最小化。
其核心思想是对随机变量进行独立采样，然后利用定义在m个样本上的平均函数来近似未知的目标函数
> ?，这里的随机变量是R复杂度，还是样本值？什么鬼？

然后衡量的目标就是经验风险最小化学的w，在目标函数上w与目标函数上最优解之间的误差 
> 这里的w与F对应的是什么？

然后通过不等式放缩即找
平常情况下的w在目标函数中的值和在样本平均中的值的差

#### 随机近似
> 利用目标函数F的有噪声观测直接优化问题，如果可以观测到目标函数值，则称为零阶随机优化；如果可以观测到目标函数的梯度，则称为一阶优化

两个例子
1. 随机梯度下降

2. 阶段随机梯度下降
阶段

### 9.3 完全信息在线学习
> 这一章以及后面就涉及到在线学习的内容了。在线学习在某种意义上可以看作是学习器与对手之间的博弈过程。完全信息在线学习能观测到全部的信息，赌博机在线学习只能观测到局部的信息。

1. 基于专家建议的预测
> 对手给出yt, 每个专家给出自己的意见，然后统帅根据某种方法综合专家的意见（线性或者加权），这种方法带有参数，共N个专家，训练T轮。
> 如何学习最优决策？目标是统帅做出决策所获得的损失和专家的损失和相近。（就是专家最牛，希望自己的决策最终能趋近于专家的决策）

2. 在线凸优化
> 对于损失函数的选择，从某种意义上感觉来说还是与函数的性质有关。
3. 在线梯度下降
> 这里分凸函数和强凸函数
4. 在线牛顿法
> 指数凹函数
5. 在线批处理转换

> 从某种意义上，可以再看看源码中怎么写的
### 9.4 赌博机在线学习
> 对于赌博机来说，学习器预测错误后并不知道正确答案。

1. 多臂赌博机
> 学习器的遗憾就是探索和利用之间的折中；一方面，为了准确均值估计需要探索;另一方面，学习器又倾向于选择最大收益的手臂。

> 注意选择不能只是选均值大的。

> 评价准则：首先最优选择是什么？事先知道答案，即选择了最优的。那么就是T*Ui; 其次，实际做的是什么？就是每轮真正的选择之和。

一种做法就是置信上界，为每个手臂维持一个均值加一个变量，注意这个变量定的方式暂时未知。

2. 线性赌博机
> 一个问题，噪声具有分布，怎么观测到噪声，噪声有什么用。

首先，这里有个目标；目标可以是最小方差。然后，可以取得一个权重，并根据这个权重找到对应的置信趋于，有置信区域找到上界。然后再根据各个标签的置信区域的上界来找到$X_{t+1}$。
然后提交观测的样本，再进行学习。
注意，一些扩展的tips，最小方差，在线最小方差。

3. 凸赌博机

在赌博机的设定下，学习器智能观测到损失函数$f_t$在决策$w_t$上的值$f(w_t)$,因此是无法直接应用梯度下降的。
所以这里引入了从函数值估计梯度的技术。
首先在x组成的空间中随机选择一个单位向量，然后学习器根据这个单位向量选择决策$w_t=z_t+\delta u_t$;同时，对手选择一个损失函数。学习器遭受损失，并更新$z_t$。值得注意的是这是就能从$z_t$近似来做，最终来做凸优化。
## 补充
#### 1.频率 vs 贝叶斯

#### 频率
> 有上帝之手的感觉，假设事物在冥冥之中服从一个分部，这个分布的参数是未知的，但是是固定的

最大似然估计MLE:
首先假定一个分布，然后基于最大期望估计得到模型的参数。参数固定。


#### 贝叶斯
> 世界是不确定的，人们对世界先有一个预判，而后通过观测数据对这个预判进行调整。我们的目标是要找到最优的描述这个世界的概率分布。参数是一个随机变量，符合一定的概率分布。

最大后验估计MAP:
对世界预判后，不断改变参数找到最适合的分布

过拟合？频率派的专有词
频率学派——最大似然估计 MLE.
> EM算法使用的就是MLE

缓解办法：
1. 添加正则化项
2. 模型集成

贝叶斯学派——最大后验估计MAP

缓解办法：
1. 引入先验，做最大后验估计？
> 认为正则化项实际是一种先验分布，比如u均值的拉普拉斯分布，0均值的高斯分布
2. 计算后验分布的积分（相当于给不同模型进行加权组合）

### 三种函数的性质

凸函数：局部最优就是全局最优
强凸：只有唯一的最优值，因为表达式中涉及到二阶值，所以此时可以看出强凸有距离的概念。强凸使得前一轮的信息可以直接用到当前论

### Nothing is more practical than a good theory!

Notations
- Input space
- Fuction: g
- Risk of g: 泛化风险
- Regression function: 回归问题
- Target function: 非回归问题
> ? 后两个是最优的function

普通的策略都是经验风险最小化来得到最优分类器，然后防止过拟合引入正则化项。

> 
所以必然考虑到基于训练集得到的分类器的误差与最小分类器得到的误差之间的关系。
> 最优分类器可能没在假设空间中。

然后根据二次bound可以bound到 
$R(g_n)-R_n(g_n)$,即generalization下最优误差与经验风险最小化误差的差值。

如果带有损失函数用来分析一致性、稳定性之类，利用Lipschitz条件进行放缩

分析$R(g_n)-R_n(g_n)$
利用Hoeffding's Inequilty
推出假设空间为有限的界

若假设空间为无限，由秧差得到R复杂度。

> 当面对具体算法，因为是凸优化问题；就可以根据分类器即函数的性质得到收敛速率之类的事情。
### 应用
未标签数据的应用：
四种，其中前三种已经有理论证明。
1. 生成式方法。
> 从有标记数据中学得参数，作用在未标记数据上。然后更新有标记数据集，进而更新参数。
2. 半监督SVM
> 一次性读入数据，不能中间添加数据。
> 不能在采样点秘籍的地方穿过
3. 图
> 标签传播，进行随机游走
4. 一组分类器
> 基于不一致性进行学习；多视图学习

### Generalization
- 证明PAC
> 写出最优分类器，然后拆分，然后做
- VC
> 打散，假设空间可自己构造
- 算R
> 假设带进入做期望
- R的性质
1. R(aH)=|a|R(H)
2. R(H1+H2)=R(H1)+R(H2)
3. R(max(h1,h2)) <= R(h1) + R(h2)
> max=$\frac{1}{2} (x+y+|x-y|)$
- 为什么对假设空间进行限制的合理？
因为误差会随假设空间的维度增大而减小
- 证明最优分类器
> 把一般的表达式写出来，证明最小值即最优
- 稳定性
> 说明有界，并且证明$\delta$可容许