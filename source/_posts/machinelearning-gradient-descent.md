---
title: machinelearning gradient descent
copyright: true
top: 0
reward: false
mathjax: true
date: 2019-09-19 13:11:31
tags:
- machinelearning
- gradient-descent
categories:
- [machinelearning, gradient-descent]
---

SGD ? 是只是针对一个样本做的吗？ 难道不是学习了随机

无偏估计：不考虑系统误差，只有随机误差，在大数情况下有效。 也就是估计式的期望等于估计值

SGD Challenges:
1. 收敛速度跟学习速率关系很大，大容易震荡，小收敛很慢
2. 学习速率对所有特征都是一样的。事实上，应该某些特征下降慢，有些快，没有考虑稀疏性
3. 容易陷入不太好局部最小或鞍点。


误差曲面，权重为多维的
梯度与等高线的关系
梯度是等高线上的法向量
梯度是数字较低的等高线指向数值较高的等高线，梯度的模是函数在这个法线方向的方向导数
常见的有两种误差曲线：
一种一维的，一种等高线的，实际上就是反映在图像上多维的等高线
动量使得收敛过程更快地通过SGD来回震荡的峡谷

改进
1. 动量： 模拟物体运动的惯性，更新时一定程度上保留之前更新的方向，同时利用当前的batch的梯度微雕最终的更新方向。增加了一定程度上的稳定性，有一定的摆脱局部最优的能力
    - 传统：首先计算当前位置的梯度，然后再更新累加的梯度方向移动
    - NAG: 先在之前累加的梯度方向上移动，然后再计算当前位置的梯度？
        > 利用了物理中动量的思想，保持总的下降方向减小震荡，也比较容易跳出不太好的局部最小 
2. 自动调节学习速率
    - Adagrad: Adagrad利用以前的梯度信息Gt判断对应的特征是否经常被更新， 平滑项用于防止分母为0
        > 考虑了之前的梯度的累加的信息，Gt是一个对角矩阵，每个对角线(i,i)的值为累加到t次迭代的对应参数wt梯度平方和，越到后面学习率会变得越来越小
        - Rmsprop: Adagrad的改进：解决Adagrad算法中学习率单调下降趋向于0的问题，把历史梯度累积窗口限定到固定的窗口
        - Adadelta: 使用均方根来估计步长，使用临近时间窗口的步长来估计当前的步长；避免了手动调借参数
            - Adam： 使用一阶和二阶矩估计
            > Adam = Rmsprop + Bias-crrection + Momentum 
                > 初始化gradient较大，导致基于速度的算法NAG发生偏离又修正过来；具有自适应学习率的算法效果想加速版的SGD,能更加稳定地解决large initial gradient问题

二阶优化方法

现实世界中所遇到的Hessian矩阵都是实对称的，实对称矩阵都能分解为实特征向量和实特征值
梯度下降无法利用含在Hessian矩阵的曲率信息。对于正曲率，梯度下降会下降得很慢；对于负曲率，梯度下降会下降得很快。当迭代点越靠近X，其搜索步长就越小，因而收敛速度越慢

牛顿法，寻找收敛速度快的无约束最优化方法，在每次迭代时，用适当的二次函数去近似目标函数f
> 更新公式，二阶和一阶的内积和？
二阶矩阵计算内容包含太多。

牛顿法考虑走了一步后，坡度是否会变得更大。要求二阶矩阵的逆，计算量巨大k^3

改进
- 共轭梯度
    > 避免求逆运算
    > 向量共轭是正交的推广，共轭方向法最多经过N步迭代，就可以到达极小值点，二次收敛性。沿共轭方向集的每个方向顺序做line search的时候，在每个方向上都不需要做重复搜索。每个方向的移动都不会影响到在另一方向上已经找到的极小值？
    CG:线性搜索并不严重依赖于线性搜索寻找该方向上和真正极小值很近的一点。
- 拟牛顿法
    > 不用二阶偏导数而构造正定对称阵
    > BFGS, 需要存储Hessian逆矩阵M, 空间复杂度O(n), 花费更少的时间改进每个线性搜索
    line search: 搜索方向d_k 已经是确定的，目标是如何在一个确定的d_k上，找到一个合适的a_k

