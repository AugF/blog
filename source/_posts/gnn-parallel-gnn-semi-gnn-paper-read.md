---
title: gnn-parallel gnn semi-gnn paper read
copyright: true
top: 0
reward: false
mathjax: true
date: 2019-09-19 12:56:18
tags:
- gnn-parallel
- gnn
- paper
categories:
- [gnn-parallel, gnn]
---
## 1. Introduction

半监督学习步骤：
1. 选取标记数据，划分为训练集、验证集和测试集
2. 根据数据进行学习、训练模型

- 模型目标函数
$$\mathcal{L} = \mathcal{L}_0 + \lambda \mathcal{L}_{reg}$$
这里， $\mathcal{L}_0 = \sum_{(\vec{x}, y)} H(\vec{x}, y)$
$\mathcal{L}_{reg}=\sum_{i,j} A_{ij} ||f(x_i)-f(x,j) ||^2 = f(X)^T (D - A) f(X)$
    > 这里 $D_{ii}=\sum_j A_{ij}$为对角矩阵, 该正则化项是二范数，所以$\lambda$为L2参数
    
    其中 $H(\vec{x}, y) = - \sum_i y'_i log y_i$， 表示交叉熵损失函数
    > 使用交叉熵损失函数式，上一步为softmax函数，即指数标准化函数; $y'_i$是即真实类别，$y_i$是对应节点最终得出来的关于某个类别的概率

    进一步，
    $$y = softmax(\hat{A}\ ReLU(\hat{A}XW^{(0)})\ W^{(1)})$$
    > y是深度神经网络预测出来的每个类别的概率的矩阵, 这里是三层的公式
    
    多层的公式即为，$y = softmax(\tilde{A}\ ReLU(\tilde{A}\ ReLU(...)\ W^{(n-1)})\ W^{(n)})$

    在这里层与层之间的更新公式有两种：

    - $X^{(l+1)} = \delta (\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} X^{(l)} W^{(l)})$
        - $\tilde{A}=A+I_N$: 这里增加考虑了自己的因素;
        - $\tilde{D}_{ii}=\sum_j \tilde{A}_{ij}$
        - $W_{(l)}$是每层的权重
        - $\delta$是激活函数，比如$ReLU(\cdot)=max(\cdot, 0)$
        - $X^{(0)}= X$, 表示每一层表示得到的表示
        ![](asserts/paper2/fig1.png)
        > 这里C是输入的channel, F是倒数第二层的输出channel
    - $X^{(l+1)} = \delta (\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} X^{(l)} W^{(l)}) + X^{(l)}$
        > 又称为+残差  Residual

$$ \mathbf{\hat{A}} = \mathbf{\tilde{D}^{-\frac{1}{2}}} \mathbf{\tilde{A}} \mathbf{\tilde{D}^{-\frac{1}{2}}} $$
$$ \hat{A} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$$
- 模型更新
    > Adam优化器，自适应梯度下降优化器 
https://www.jianshu.com/p/aebcaf8af76e

## 3. 公式的由来
[spectual based](https://arxiv.org/pdf/1211.0053.pdf)

[Hammond.](https://hal.inria.fr/inria-00541855/document)
> 近似方法推导的来源

[Cheby方法](https://papers.nips.cc/paper/6081-convolutional-neural-networks-on-graphs-with-fast-localized-spectral-filtering.pdf)


## 4. 相关工作
1. 内存扩展
2. 使用mini batch 随机梯度下降

### 4.1 基于图的半监督学习

拉普拉斯矩阵： 标签传播、多种正则化、深度半监督表示

注意力机制： DeepWalk,
> skim-gram model, 简单来说就是，当要看当前单词时，向前或向后看几个单词
> 通常这些做法有种更多的步骤或者宽度优先模式，所以需要考虑用哪一步进行优化才是最恰当的

### 4.1 关于图的神经网络

2009的框架重复利用了收缩的想法进行传播误差直到最终节点的表示到达一个稳定的状态

2015年的做法介绍了一种类似卷积的传播规则作用在图上，并且针对于图级别的分类。但是可以发现，因为卷积要求有着固定的邻居。所以这种做法是不符合当节点的度有着很广的分布的情况。
> 这篇文章会重新考虑计算邻接矩阵，所以不会担心有这种因素的出现

2016,1-DNN, 引入了排序的想法
本文的方法，2014,简单，可扩展

## 5. 实验

### 5.1 数据集
**类别**
1. 引用网络
Citeseer, Cora, Pubmed
2. 知识图
NELL, 本身属于二部图

**内容**
数据： 数据类别， 节点， 边（这里会进行定义）， classes:节点的类别， features:输入的channels， Label rate: 用作训练的数据集

![](asserts/paper2/dataset.png)

**预处理**
- 引用网络
无向图，不用处理，二进制邻接举证；每个类只使用了20个标记，使用了全部的特征
> 1. 是如何筛选出训练集和测试集的？
> 
> 2. 是否保证了训练集中各个标记的样本的比例相同？


- NELL
有向图，预处理同2016. 做法将(e1, r, e2)拆分为(e1,r1), (e2, r2) `也就是将节点拆分成了很多片, 这样从某种程度就可以使得一条边可以被两个点同时共用` 所以也就使得得到的节点表示是稀疏向量，one-hot作为特征，所以产生了61278维，`这里可以注意到，one-hot每一个特征所拥有的取值即其位数`，如果两个节点中存在边的话，则说明边是存在的，则对应邻接矩阵中置为1

- Random graphs
我们模拟各种尺寸的随机图数据集，并在每轮中测试训练时间。
一般的做法时，随机地为N个节点赋予2N条边。对于每个节点的特征初始化为$I_N$, 表示每个节点并没有什么特征。并为每个节点置一个假标记$Y_i = 1$
> 目的是为了什么？ 有什么用？

### 5.2 实验部分

首先是3层的网络，附录B中有10层的网络。
data split技术 [Yang, 2016](https://arxiv.org/pdf/1603.08861.pdf)
> ？ 这里是1000个样本做测试，500个样本做交叉验证的意思吗？

3层的网络，超参数：
- dropout
- L2
- number of units
- optimizer
> 优化器，就是采用什么样的梯度下降方式

> 这里不使用验证集作为训练，那还是用的交叉验证的方法吗？？

在Cora上优化得到的超参数直接用到Citeseer, Pumbed。 训练中优化器使用的是Adam， 学习率为0.01
> Adam优化器，自适应梯度下降优化器 
https://www.jianshu.com/p/aebcaf8af76e

停止条件：最大训练轮数200轮，如果交叉验证集误差连续10次不下降则停止训练。
权重初始化和特征向量标准化方法：[Glorot & Bengio 2010](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
隐藏层单元：32个
忽略正则化项
> 隐藏层单元的设置除了对业务的理解，还有什么其他的建议点？
> 否则一般有经验公式

### 5.3 baselines
> 注意到这里baselines比较了前面提到的基于图的半监督学习中所有目前有的论文的模型。并且可以忽略其中不适合的模型

忽略了TSVM, 不适用于大规模分类

进一步与Lu提出的迭代分类算法ICA进行了比较，结合两个逻辑回归分类器，一个用于本地接地那特征，一个用于使用局部特征和聚合算子的关系进行分类。首先，使用所欲标记的训练集训练本地分类器并使用它用于引导为未标记节点的类标记。然后，运行迭代分类器，随机节点排序，对所有未标记的节点进行10次迭代。`这里是为了做什么`
L2交叉验证和聚集函数的选择都是基于每个数据集的验证集的性能分别进行选择
选择了Planetoid，选择了它们模型最好的变体

## 6. 结果
ICA随机选择100论随机节点排序运行的平均准确率



## 7. 贡献
主要有两个贡献： 
1. 提出了一种新的梯度计算公式
2. 这个模型可以快速用于大规模的半监督的节点分类中


## 一些超参数
    - dropout
    > 每层都可以设置，[0,1], 表示以多大的概率使当前节点停止工作，避免过拟合
    - L2 正则化系数
    > ?为啥有第一层， 这里的理解不应该是$\lambda$吗
    - number of units for each hidden layer 隐藏层节点个数
    > 隐藏层表示的特征数？ 对吗？
    - 学习率
    > $\eta$: 梯度下降的步长


## 很神奇的地方
    - filter公式的推出
    - Tk的近似
    - K=1, $\lambda_{max}$的取值

## 一些定义
$G = (\mathcal{V}, \mathcal{E})$
顶点数和边数

F-- output layer的channel个数
C-- input layer的channel个数
H-- input layer的输出，channel, hidden layer的输入
## 一些不懂的点

训练集
> 由损失函数，特定的评价指标；一般看的训练误差也就是损失函数的误差。

交叉验证集：用于评价泛化性如何，是否产生了过拟合现象，然后交叉验证的准确率可以进行反应
> 如果表现得很低，那么有可以在训练集上进行重新训练；再次交叉验证，唯一注意的是不能混用。
> 然后，一般来说采用的是K交叉验证，即将数据集分为K份，最终的做法是取这些值的平均值来做

> ??? K交叉取平均会不会混淆，因为在用的时候，实际是在多次训练时，虽然本次没有用到数据集，但是在下一次肯定用到了，那么是不是有问题

> 如果是独立的训练，那么又是取哪个值来做呢？ 注意关注一下交叉验证.
> https://blog.csdn.net/lhx878619717/article/details/49079785
> 按这里的说法，就是分为k份，然后每次训练集是独立的，最终选取的目标其实就是测试误差最小的模型为最优模型。该模型的参数就是最终的参数
> 突然想起，之前的学习也说明了这里的问题

测试集：最终的真实结果，未知
> 从某种意义上讲，交叉验证的实际结果应该和测试集上表现尽量相同，也就是说性能度量指标基本上就是在这里用的。
> 一般来说实验的精确率就是用交叉验证集和测试集的交过来反映来看
> 但是这里不一定相等，因为存在训练集过小等因素

> 比如，查准率，查全率，F1

https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E9%A9%97%E8%AD%89


学习率：反向传播中的步长
L2正则化参数：L2的系数


矩阵
> 一般来说，把向量定义为列向量；为什么？因为人们就作用来说，还是习惯左作用。
> 比如A对应的线性映射为Ax
https://www.zhihu.com/question/26304877


## 不懂的地方

表达式不懂

hidden层的激活函数是怎么用的？


交叉熵和MSE最小平方法都是用来做损失函数的

几个概念：
- 损失函数：单个样本
- 代价函数：针对总体
- 目标函数：通常会考虑正则化项

所以这里正则化项系数就是目标函数中使用的是哪种正则化项，然后就对应于哪个参数

其实，本身一般来说是把问题经过抽象变成最优化问题的，这里的最优化问题可以分为有约束最优化问题，无约束最优化问题以及其他；
然后，本身最优化问题有多种解的，比如模拟退火算法、遗传算法、蚁群算法、图割算法。
但是，在机器学习中，一直长学的求导占了主要的位置，所以考虑的就是使用求导法，也就是所谓的梯度下降，然后所以每次在考虑很多的问题都想要保证凸函数，因为这样才能保证取得全局最优解

https://blog.csdn.net/liujiboy/article/details/78078042

然后，又怎么转化到学习上去了呢？就是为什么是大量的样本上去了呢。其实，本身不管是什么事情，除非是固定的数学问题，都是一个学习的过程。那么，就永远不会到达最理想的状态，所以就是学习本身，那么引入正则化项也合理了，就是为了避免进行过拟合。

> 更深入地从KL散度了解交叉熵
https://blog.csdn.net/tsyccnh/article/details/79163834

sigmoid函数，1/(1+e^(-x)) 将R映射到了[0,1]


https://zhuanlan.zhihu.com/p/3824176

